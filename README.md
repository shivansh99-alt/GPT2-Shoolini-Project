# GPT2-Shoolini-Project
This project demonstrates pre-training a GPT-2 language model from scratch on a custom dataset related to Shoolini University. Using Hugging Faceâ€™s Transformers library, we train a small GPT-2 model without pretrained weights to learn university-specific vocabulary, style, and content. This process is an example of domain-specific language model pre-training for NLP applications.
